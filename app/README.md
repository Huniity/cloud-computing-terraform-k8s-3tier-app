
 # Project Hub - Course Management System

![Django](https://img.shields.io/badge/Django-5.1.7-green)
![Python](https://img.shields.io/badge/Python-3.12.9%2B-blue)
![Kubernetes](https://img.shields.io/badge/Kubernetes-Latest-blue)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-336791)

## Overview

Project Hub is a full-featured course management system built with Django, deployed on Kubernetes. It enables seamless interaction between Students and Mentors through an intuitive web interface with role-based access control, course enrollment, and administrative tools.

### Key Features

- **User Authentication** - Signup, login, and role-based access control
- **Course Management** - Create, manage, and enroll in courses
- **Student Dashboard** - View enrolled courses and track progress
- **Mentor Dashboard** - Manage created courses and students
- **RESTful API** - Built with Django REST Framework
- **Admin Interface** - Django admin for administrative tasks
- **HTTPS Support** - Self-signed certificates included

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User's Browser                              â”‚
â”‚                   (localhost:8443 HTTPS)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NGINX Ingress                                 â”‚
â”‚              (TLS Termination, Routing)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                       â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Frontend   â”‚      â”‚   Backend    â”‚  â”‚   Database   â”‚
    â”‚   (Nginx)    â”‚      â”‚   (Django)   â”‚  â”‚  (PostgreSQL)â”‚
    â”‚              â”‚      â”‚              â”‚  â”‚              â”‚
    â”‚ HTML/CSS/JS  â”‚      â”‚  REST API    â”‚  â”‚   Tables     â”‚
    â”‚ (Port 80)    â”‚      â”‚  (Port 8000) â”‚  â”‚  (Port 5432) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Prerequisites

- Docker
- kubectl
- Minikube (for local development)
- OpenSSL (for TLS certificate generation)

## Installation & Setup

### Quick Start (One Command)

```bash
make setup
```

This runs: `install` â†’ `deploy` â†’ `test`

### Step-by-Step Setup

**Step 1: Install & Prepare**
```bash
make install
```
- Starts minikube
- Enables ingress addon
- Builds Docker images
- Creates TLS certificate

**Step 2: Deploy Application**
```bash
make deploy
```
- Applies all Kubernetes configs
- Deploys database, backend, frontend
- Sets up port forwarding
- Enables HTTPS access

**Step 3: Test**
```bash
make test
```
- Verifies all pods are running
- Tests HTTP endpoints
- Tests API connectivity

### Access the Application

After setup completes, access:

| Service | URL | Port |
|---------|-----|------|
| Frontend (HTTP) | http://localhost:8080 | 8080 |
| Backend API | http://localhost:8000/api | 8000 |
| Frontend (HTTPS) | https://localhost:8443 | 8443 |
| Admin Panel | http://localhost:8000/admin | 8000 |

**Note:** HTTPS uses self-signed certificate. Accept the browser warning on first access.

## Available Commands

```bash
make install    # Setup minikube and build images
make deploy     # Deploy to Kubernetes
make test       # Run tests
make cleanup    # Remove all resources
make setup      # Full setup (install + deploy + test)
make help       # Show all commands
```

Or use the interactive menu:
```bash
bash scripts/setup.sh
```

## Project Structure

```
â”œâ”€â”€ backend/                 # Django REST API
â”‚   â”œâ”€â”€ learning_hub/       # Django settings
â”‚   â”œâ”€â”€ my_course/          # Course app
â”‚   â”œâ”€â”€ cli/                # CLI tools
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/               # Nginx + HTML/CSS/JS
â”‚   â”œâ”€â”€ html/              # HTML pages
â”‚   â”œâ”€â”€ css/               # Stylesheets
â”‚   â”œâ”€â”€ js/                # JavaScript
â”‚   â”œâ”€â”€ nginx.conf         # Nginx configuration
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ database/              # PostgreSQL
â”‚   â”œâ”€â”€ init.sql          # Initial schema
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ statefulset.yaml
â”‚
â”œâ”€â”€ ingress/              # Kubernetes ingress + TLS
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”œâ”€â”€ tls.crt
â”‚   â””â”€â”€ tls.key
â”‚
â”œâ”€â”€ scripts/              # Automation scripts
â”‚   â”œâ”€â”€ install.sh
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â”œâ”€â”€ test.sh
â”‚   â”œâ”€â”€ cleanup.sh
â”‚   â””â”€â”€ setup.sh
â”‚
â””â”€â”€ Makefile.local        # Make commands
```

## Cleanup

Remove all resources and reset:

```bash
make cleanup
```

## Troubleshooting

### Port Already in Use
```bash
pkill -f "kubectl port-forward"
```

### View Logs
```bash
bash scripts/setup.sh  # Select option 6
# Or directly:
kubectl logs -l app=frontend
kubectl logs -l app=backend
kubectl logs -l app=database
```

### Restart Services
```bash
kubectl rollout restart deployment/frontend
kubectl rollout restart deployment/backend
```

## Technologies Used

- **Frontend:** HTML5, CSS3, JavaScript, Nginx
- **Backend:** Django 5.1, Django REST Framework, Python 3.12
- **Database:** PostgreSQL 15
- **Infrastructure:** Kubernetes, Docker, Minikube
- **Security:** TLS/HTTPS, Self-signed certificates

## License

This project is licensed under the MIT License.
  This commands creates the .env file so you don't have to. Builds the container, migrates data, injects the data from .Json in the Postgres DB, opens the browser on the index of the app and finally re-opens logs.
  You can at anytime, log in or logout and create new user on the website. New comers always get Student role, to grants Mentor access you can proceed to Django Administration or using the CLI commands on the CLI section of this README.md.
  Following links have profiles already created so you can see how this SAaS works, from Student side to Mentor side.
  Following links available in case you need too and respective credentials:

  - http://localhost:8000/
  Access to following users:
    - User: Student
      - Password: useruser
      - Grants access to a Student user, with respective roles to see courses, enroll and check where you are enrolled.
    - User: Mentor
      - Password: useruser
      - Grants access to a Mentor user, with respectve roles to create, see own courses and who is enrolled to them. 

  - http://localhost:8000/admin
    - User: admin
      - Password: admin
      - Grants access to admininstration of Django.

  - http://localhost:8080/
  Access to Postgres:
    - System: PostgreSQL
    - Server: database
    - Username: postgres
    - Password: qwerty
    - Database: hub_db



### CLI Commands for Admin use Only
- `docker compose run --rm web poetry run python cli/cli.py super`
- `docker compose run --rm web poetry run python cli/cli.py create-user` 
- `docker compose run --rm web poetry run python cli/cli.py delete-user <username>`
- `docker compose run --rm web poetry run python cli/cli.py group-user <username> <groupname>`
- `docker compose run --rm web poetry run python cli/cli.py reset-group <username>`
- `docker compose run --rm web poetry run python cli/cli.py list-course`
- `docker compose run --rm web poetry run python cli/cli.py list-course --short`
- `docker compose run --rm web poetry run python cli/cli.py list-course --save`


### Requirements 
- Python 3.12.9+
- Docker 4.39.0 for Container compatilibty using Debian Docker in Docker.
- PostgreSQL
- Typer

### Dependencies
```toml
dependencies = [
    "django (>=5.1.7,<6.0.0)",
    "uvicorn (>=0.34.0,<0.35.0)",
    "psycopg2-binary (>=2.9.10,<3.0.0)",
    "typer (>=0.15.2,<0.16.0)",
    "whitenoise (>=6.9.0,<7.0.0)"
]
[tool.poetry.group.dev.dependencies]
pytest-django = "^4.10.0"
```

### How did I structured my project
![Diagram](HUB_Diagram.png)

### Project Structure
```projecthub/
â”œâ”€â”€ .devcontainer/       
â”œâ”€â”€ my_project/              
â”‚   â””â”€â”€ templates/          
â”œâ”€â”€ my_course/       
â”‚   â”œâ”€â”€ migrations/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ admin.py
â”‚   â”œâ”€â”€ apps.py
â”‚   â”œâ”€â”€ forms.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ urls.py
â”‚   â””â”€â”€ views.py
â”œâ”€â”€ static/   
â”œâ”€â”€ projecthub/     
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ urls.py
â”‚   â”œâ”€â”€ asgi.py
â”‚   â””â”€â”€ wsgi.py
â”œâ”€â”€ .env
â”œâ”€â”€ compose.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ manage.py
â””â”€â”€ ...other root files ```


dev 1 nodes, staging 2 nodes, prod 3 nodes
terraform plan -var-file=spotify.tfvars -out spotify.plan
terraform workspace new spotify
cluster config (client, nodes, env, addons)

act as a DevOps in your answers:

Got it ğŸ‘
Iâ€™ll answer as a DevOps engineer from here onâ€”practical, production-focused, and biased toward automation, reliability, security, and scalability.

Whenever youâ€™re ready, tell me what you need help with, for example:

CI/CD pipelines (GitHub Actions, GitLab CI, Jenkins)

Cloud (AWS, GCP, Azure)

Containers & orchestration (Docker, Kubernetes, Helm)

IaC (Terraform, Pulumi, CloudFormation)

Monitoring & logging (Prometheus, Grafana, ELK, OpenTelemetry)

Linux, networking, security, or incident troubleshooting

Whatâ€™s the task?

i was using k8s in my project. using minikube cache add i was using local custom images to use them in my deployments. I'm moving now to terraform and i wont be able to use any kubectl cmd. My teacher suggested to see how to build custom local docker images to have a look at null_resource or local_exec to help me. Can you give me a hand

Alright, DevOps hat on ğŸ§¢
Youâ€™re hitting a very common transition pain point when moving from â€œkubectl + minikubeâ€ to Terraform-managed workflows.

Letâ€™s break this down cleanly and practically, then Iâ€™ll give you working patterns.

The core problem (whatâ€™s changing)
Before (imperative)
minikube cache add my-image:latest
kubectl apply -f deployment.yaml


You relied on:

Local Docker images

kubectl imperatively pushing them into Minikubeâ€™s Docker daemon

Now (declarative with Terraform)

Terraform cannot magically build Docker images

Terraform should not manage runtime cluster state directly

You still need a way to:

Build a local Docker image

Make it available to Minikube

Reference it in a Kubernetes Deployment

The right mental model (important)

Terraform:

âœ… Good at orchestrating

âŒ Not good at building artifacts

So the solution is:

Use Terraform to trigger Docker builds, not replace Docker.

This is exactly where null_resource + local-exec fits.

Option 1 (Recommended for Minikube): Build directly inside Minikube Docker

Minikube has its own Docker daemon.

If Terraform runs:

eval $(minikube docker-env)
docker build -t my-app:latest .


Then:

Image is immediately available

âŒ No minikube cache add

âŒ No registry needed

Terraform example (clean and minimal)
resource "null_resource" "build_image" {
  triggers = {
    dockerfile = filesha256("${path.module}/Dockerfile")
    src        = filesha256("${path.module}/app/main.py")
  }

  provisioner "local-exec" {
    command = <<EOT
      eval $(minikube docker-env)
      docker build -t my-app:latest .
    EOT
  }
}

Why this works well

triggers ensure rebuilds only when files change

Image lives inside Minikube

No registry complexity

Kubernetes deployment referencing the image
resource "kubernetes_deployment" "app" {
  metadata {
    name = "my-app"
  }

  spec {
    replicas = 1

    selector {
      match_labels = {
        app = "my-app"
      }
    }

    template {
      metadata {
        labels = {
          app = "my-app"
        }
      }

      spec {
        container {
          name  = "my-app"
          image = "my-app:latest"

          image_pull_policy = "IfNotPresent"
        }
      }
    }
  }

  depends_on = [null_resource.build_image]
}


ğŸš¨ Important

image_pull_policy = "IfNotPresent"


Otherwise Kubernetes will try to pull from Docker Hub.

Option 2: Using Docker provider (cleaner, but slightly heavier)

Terraform has a Docker provider.

Build image
provider "docker" {}

resource "docker_image" "app" {
  name = "my-app:latest"
  build {
    context = "${path.module}"
  }
}

BUTâ€¦

Docker provider builds on host Docker

Minikube wonâ€™t see it unless Docker is shared

So youâ€™d still need:

eval $(minikube docker-env)


Which makes Option 1 simpler for learning.

Option 3 (More â€œreal worldâ€): Local registry (FYI)

In production-like setups:

Build image

Push to:

Local registry

Docker Hub

GHCR

Kubernetes pulls image

This is overkill for your course, but good to know.

What your teacher is guiding you toward

Your teacher mentioning:

null_resource

local_exec

Means:

â€œUse Terraform to orchestrate build steps, not replace kubectlâ€

This is exactly correct DevOps thinking.

Best practices (exam & real-world friendly)

âœ… Keep builds outside Terraform when possible
âœ… Use null_resource only for:

Local builds

Bootstrapping

Learning environments

âŒ Donâ€™t use Terraform as a CI tool in real prod
âŒ Donâ€™t rely on kubectl inside Terraform unless unavoidable

TL;DR (what you should implement)

Use Minikube Docker daemon

Build image via null_resource + local-exec

Set image_pull_policy = "IfNotPresent"

Add depends_on

If you want, I can:

Refactor this into a clean module

Show how to replace this later with CI/CD

Show how to do this without null_resource (advanced)

Just tell me ğŸ‘Œ

explain me option 3

Alright, letâ€™s go full DevOps-style explanation and demystify Option 3: Local registry.
This is closer to real-world workflows, even if itâ€™s heavier than what your course strictly needs.

Option 3: Using a Local Docker Registry (DevOps-grade approach)
Why this exists (the â€œwhyâ€ matters)

In real environments:

Kubernetes never uses local Docker images

Nodes pull images from a registry

CI builds â†’ pushes image â†’ cluster pulls

Minikube is special because it runs locally, but production clusters behave like this:

[ CI / Terraform trigger ]
        â†“
[ Docker build ]
        â†“
[ Docker Registry ]
        â†“
[ Kubernetes pulls image ]


So Option 3 emulates production even on your laptop.

What is a local registry?

A Docker registry running locally, usually:

localhost:5000

Or Minikubeâ€™s built-in registry addon

You:

Build image

Tag it for the registry

Push it

Kubernetes pulls it

Architecture (mental model)
+-------------------+
|   Terraform       |
|                   |
|  null_resource    |
|    local-exec     |
+---------+---------+
          |
          v
+-------------------+
| Docker build      |
| Docker push       |
+---------+---------+
          |
          v
+-------------------+
| Local Registry    |
| localhost:5000    |
+---------+---------+
          |
          v
+-------------------+
| Kubernetes Pod    |
| pulls image       |
+-------------------+

Two ways to do Option 3
3Aï¸âƒ£ Minikube registry addon (BEST for Minikube)

Minikube can run a registry inside the cluster.

Enable it (one time)
minikube addons enable registry


Registry will be available at:

localhost:5000

Build & push image
docker build -t localhost:5000/my-app:latest .
docker push localhost:5000/my-app:latest


Kubernetes can now pull it.

Terraform automation (what you actually asked about)
resource "null_resource" "build_and_push" {
  triggers = {
    dockerfile = filesha256("${path.module}/Dockerfile")
    src        = filesha256("${path.module}/app/main.py")
  }

  provisioner "local-exec" {
    command = <<EOT
      docker build -t localhost:5000/my-app:latest .
      docker push localhost:5000/my-app:latest
    EOT
  }
}

Kubernetes deployment
resource "kubernetes_deployment" "app" {
  metadata {
    name = "my-app"
  }

  spec {
    replicas = 1

    selector {
      match_labels = {
        app = "my-app"
      }
    }

    template {
      metadata {
        labels = {
          app = "my-app"
        }
      }

      spec {
        container {
          name  = "my-app"
          image = "localhost:5000/my-app:latest"
        }
      }
    }
  }

  depends_on = [null_resource.build_and_push]
}


âœ… No imagePullPolicy hacks needed
âœ… Very close to production behavior

3Bï¸âƒ£ External local registry container (generic Docker way)

Run a registry yourself:

docker run -d -p 5000:5000 --name registry registry:2


Same build/push logic:

docker build -t localhost:5000/my-app:latest .
docker push localhost:5000/my-app:latest


âš ï¸ For Minikube youâ€™ll need to:

Expose registry to the cluster

Configure insecure registry

This is why 3A is preferred.

Why Option 3 is â€œmore DevOps correctâ€
Aspect	Option 1	Option 3
Local-only	âœ…	âŒ
Prod-like	âŒ	âœ…
Registry flow	âŒ	âœ…
CI/CD ready	âŒ	âœ…
Complexity	Low	Medium
When should YOU use Option 3?

Use Option 3 if:

Your teacher wants production-like workflows

You want to understand real CI/CD pipelines

You want to avoid Minikube-specific hacks

Avoid it if:

Youâ€™re just learning Terraform basics

You want the fastest iteration

Real-world note (important)

In real companies:

Terraform does NOT build images

CI (GitHub Actions / GitLab CI) does:

docker build

docker push

Terraform only references image tags

What youâ€™re doing here is acceptable for learning only.

TL;DR

Option 3 = Build â†’ Push â†’ Pull

Uses a local registry (localhost:5000)

Most realistic Kubernetes workflow

Slightly more setup, much better mental model

If you want, next I can:

Show how to convert this into a CI pipeline

Compare this with Helm

Explain why Terraform + Kubernetes provider has limits

Just tell me ğŸ‘Œ

remember i'm using terraform ok, not k8s

ğŸ‘ Got it â€” Terraform-first mindset, no Kubernetes CLI usage, and Iâ€™ll stay strict about that.

Let me re-explain Option 3 (local registry) purely from a Terraform/DevOps perspective, without slipping into â€œKubernetes userâ€ language.

Option 3 â€” Local Registry, but Terraform is the orchestrator

You are not â€œusing Kubernetesâ€ manually.
You are:

Using Terraform to:

Build a Docker image

Push it to a registry

Declare infrastructure/state that consumes that image

Terraform is coordinating tools, not replacing them.

Why this option exists (Terraform reality check)

Terraform:

âŒ Cannot build container images natively

âŒ Cannot cache images like minikube cache add

âœ… Can orchestrate external steps

A registry becomes the contract between:

Image build step

Runtime platform (Kubernetes provider, Nomad, etc.)

This is exactly how Terraform is used in real DevOps stacks.

Terraform-only flow (no kubectl, no minikube cache)
Terraform apply
â”‚
â”œâ”€â”€ null_resource (local-exec)
â”‚     â”œâ”€â”€ docker build
â”‚     â””â”€â”€ docker push
â”‚
â””â”€â”€ kubernetes_* resources
      â””â”€â”€ reference image from registry


Terraform controls order and dependencies.

Step 1 â€” Registry (assumed, not managed by Terraform)

For learning:

Registry exists at localhost:5000

It could be:

Minikube addon

Docker registry container

Remote registry (GHCR, Docker Hub)

Terraform does not care â€” it just pushes to it.

Step 2 â€” Build & push via Terraform (key part)

This is the Terraform-native way to do it.

resource "null_resource" "image" {
  triggers = {
    dockerfile = filesha256("${path.module}/Dockerfile")
    source     = filesha256("${path.module}/src/main.go")
  }

  provisioner "local-exec" {
    command = <<EOT
      docker build -t localhost:5000/my-app:latest .
      docker push localhost:5000/my-app:latest
    EOT
  }
}

Why this is Terraform-correct

null_resource = procedural step

triggers = idempotency

Terraform state tracks when rebuilds happen

No imperative CLI tools like kubectl

Step 3 â€” Terraform resource consumes the image

Whether itâ€™s:

kubernetes_deployment

helm_release

nomad_job

Terraform only references the image by name.

resource "kubernetes_deployment" "app" {
  metadata {
    name = "my-app"
  }

  spec {
    template {
      spec {
        container {
          name  = "app"
          image = "localhost:5000/my-app:latest"
        }
      }
    }
  }

  depends_on = [null_resource.image]
}


This is declarative infrastructure, not cluster manipulation.

Why this satisfies your teacherâ€™s constraint

Your teacher said:

â€œLook at null_resource or local_execâ€

This pattern:

âœ” No kubectl

âœ” No minikube cache

âœ” Fully Terraform-driven

âœ” Reproducible

âœ” Matches real CI/CD logic

Youâ€™re doing exactly what they expect.

What this teaches you (important for exams/interviews)

Terraform is not a build tool

Artifacts live outside Terraform

Registries decouple build from deploy

Declarative infra + procedural glue is normal

This is senior DevOps thinking, not beginner stuff.

Common mistakes (avoid these)

âŒ Running kubectl inside local-exec
âŒ Letting Terraform rebuild images every apply
âŒ Using latest without understanding cache
âŒ Treating Terraform as CI

When this pattern disappears (real world)

Later, youâ€™ll move to:

CI:
  docker build
  docker push

Terraform:
  reference image:tag


Terraform will stop building images entirely.

TL;DR

You are using Terraform, not Kubernetes

Registry is the contract

null_resource + local-exec is the glue

This is the correct DevOps pattern

If you want next, I can:

Rewrite this using Docker provider

Show how to avoid null_resource entirely

Explain how examiners expect you to justify this design

Just say the word ğŸ‘Œ

in option 1, explain me the triggers how they work, what should they contain

Perfect question. This is exactly where most people misuse null_resource.

Iâ€™ll explain this as a DevOps + Terraform practitioner, very concretely and focused on Option 1.

triggers in null_resource â€” what they REALLY do
The short version

triggers tell Terraform when a null_resource must be recreated.

Since provisioners only run:

on create

on destroy

triggers are the only mechanism to re-run your local-exec logic.

Why triggers are mandatory in Option 1

Option 1:

docker build -t my-app:latest .


Without triggers:

Terraform runs it once

Code changes âŒ do not rebuild image

Dockerfile changes âŒ do not rebuild image

Thatâ€™s unacceptable even for learning.

How Terraform sees a null_resource

Terraform does not understand:

Docker

Images

Source code

It only understands inputs â†’ state â†’ diffs.

So you must give it inputs that change when your image should change.

Thatâ€™s what triggers are.

How triggers work internally

Example:

resource "null_resource" "build_image" {
  triggers = {
    dockerfile = filesha256("${path.module}/Dockerfile")
  }

  provisioner "local-exec" {
    command = "docker build -t my-app:latest ."
  }
}


Terraform behavior:

Computes SHA256 of Dockerfile

Stores it in state

Next terraform apply:

If hash is the same â†’ do nothing

If hash changed â†’ destroy & recreate resource

Recreate = rerun local-exec

This is the entire mechanism.

What triggers should contain (Option 1)
Rule of thumb (important)

Include everything that affects the Docker image output

âœ… Common trigger inputs
1. Dockerfile (mandatory)
dockerfile = filesha256("${path.module}/Dockerfile")

2. Application source files
app = filesha256("${path.module}/app/main.py")


Or if multiple files:

app = filesha256(join("", fileset("${path.module}/app", "**")))


(Weâ€™ll refine this in a second.)

3. Build args (if used)
build_args = var.build_env

Handling multiple source files (important detail)

Terraform cannot hash directories directly.

âŒ Wrong
filesha256("${path.module}/app")

âœ… Correct pattern
locals {
  app_files = fileset("${path.module}/app", "**")
  app_hash  = sha256(
    join("", [
      for f in local.app_files :
      filesha256("${path.module}/app/${f}")
    ])
  )
}

resource "null_resource" "build_image" {
  triggers = {
    dockerfile = filesha256("${path.module}/Dockerfile")
    app        = local.app_hash
  }

  provisioner "local-exec" {
    command = "docker build -t my-app:latest ."
  }
}


This ensures:

Any file change

Any file add/remove
Triggers rebuild.

What triggers should NOT contain

âŒ Random values
âŒ Timestamps
âŒ uuid()
âŒ Anything that changes on every apply

Example of what NOT to do:

triggers = {
  always = timestamp()
}


This causes:

Rebuild on every terraform apply

Breaks idempotency

Terraform anti-pattern

Why triggers force recreate (not update)

null_resource has:

No real lifecycle

No â€œupdateâ€ phase

So Terraform does:

destroy â†’ create


Thatâ€™s fine because:

Image build is stateless

No real resource is destroyed

How this ties into Option 1 (Minikube Docker daemon)

Your full Option 1 logic:

resource "null_resource" "build_image" {
  triggers = {
    dockerfile = filesha256("${path.module}/Dockerfile")
    app        = local.app_hash
  }

  provisioner "local-exec" {
    command = <<EOT
      eval $(minikube docker-env)
      docker build -t my-app:latest .
    EOT
  }
}


Terraform guarantees:

Image is rebuilt only when needed

Deployment depends on image existence

Exam / teacher-level explanation (memorize this)

triggers are used to convert external state (files, source code, configs) into Terraform-trackable inputs so that null_resource provisioners can be re-executed deterministically.

That sentence alone will score points.

TL;DR

null_resource runs provisioners only on create/destroy

triggers control when recreation happens

Hash everything that affects the image

Never use random or time-based triggers

This is how Terraform stays declarative

If you want next, I can:

Show a minimal trigger set vs production-grade

Explain why null_resource is considered â€œlast resortâ€

Compare this with Docker provider triggers

Just say ğŸ‘Œ

so what null_resource do, what trigger do, what local exec do

Perfect â€” this is the foundational Terraform question, and once you get this, everything else clicks.

Iâ€™ll explain it cleanly, in DevOps terms, no Kubernetes noise.

1ï¸âƒ£ What null_resource does
Definition (Terraform-level)

null_resource is a Terraform resource with no real infrastructure.

It exists only to:

Participate in Terraformâ€™s dependency graph

Have lifecycle

Run provisioners

Think of it as a placeholder node in Terraformâ€™s execution plan.

Why it exists

Terraform normally manages:

Servers

Networks

Clusters

Load balancers

But sometimes you need to:

Run a script

Build an artifact

Bootstrap something external

null_resource gives Terraform something to attach behavior to.

Mental model
Terraform graph:
  resource A
      â†“
  null_resource (does something procedural)
      â†“
  resource B


No infra is created â€” only side effects happen.

Key properties

No provider interaction

No physical resource

Only meaningful because:

It has state

It can be depended on

2ï¸âƒ£ What triggers do
Definition

triggers are a set of values Terraform tracks in state.

If any value changes, Terraform:

Destroys the null_resource

Recreates it

That recreation is what causes provisioners to rerun.

Why triggers exist

Provisioners:

Run once on create

Do not rerun automatically

triggers convert external changes (files, configs, vars) into something Terraform understands.

Example
triggers = {
  dockerfile = filesha256("Dockerfile")
}


Terraform logic:

Store hash in state

Next apply:

Same hash â†’ no change

Different hash â†’ recreate resource

Important DevOps insight

Terraform does not watch files.
You explicitly tell it what matters.

What triggers should represent

â€œAnything that should cause this action to re-runâ€

For Docker builds:

Dockerfile

Source code

Build arguments

Version tags

3ï¸âƒ£ What local-exec does
Definition

local-exec is a provisioner that runs a command on the machine executing Terraform.

Not on:

Kubernetes

Cloud VM

Cluster node

On your local system or CI runner.

What itâ€™s used for

docker build

docker push

curl

bash scripts

Bootstrapping external tools

Example
provisioner "local-exec" {
  command = "docker build -t my-app ."
}


Terraform behavior:

Runs this only when the resource is created

Not on every apply

Key limitation (very important)

No rollback

No output tracking

No idempotency guarantees

Terraform assumes:

â€œIf the command exits 0, it worked.â€

How they work together (the full picture)
resource "null_resource" "build_image" {
  triggers = {
    dockerfile = filesha256("Dockerfile")
    app        = local.app_hash
  }

  provisioner "local-exec" {
    command = "docker build -t my-app ."
  }
}

Step-by-step execution

Terraform evaluates triggers

Sees change â†’ marks resource for recreation

Destroy old null_resource

Create new null_resource

local-exec runs

State updated with new trigger values

One-sentence summary (exam gold)

null_resource: a Terraform lifecycle wrapper for non-infrastructure actions

triggers: change detectors that control when those actions re-run

local-exec: the mechanism that performs the action locally

DevOps best-practice warning (important)

null_resource + local-exec is:

âœ… OK for learning

âœ… OK for bootstrapping

âŒ Bad for long-term production builds

In real systems:

CI builds images

Terraform only references outputs

TL;DR
Component	Purpose
null_resource	Lifecycle & dependency node
triggers	Decide when to rerun
local-exec	Execute local command

If you want next, I can:

Show a diagram with lifecycle

Explain why HashiCorp discourages provisioners

Rewrite this using pure Terraform providers

Just say ğŸ‘Œ
